{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoff/anaconda3/envs/penguin/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.lit_wav2vec2 import LitWav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitWav2Vec2('large_lv60k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_fpath</th>\n",
       "      <th>tgts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/media/geoff/datasets/soapies_balanced_corpora...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/media/geoff/datasets/soapies_balanced_corpora...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/media/geoff/datasets/soapies_balanced_corpora...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/media/geoff/datasets/soapies_balanced_corpora...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/media/geoff/datasets/soapies_balanced_corpora...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         audio_fpath  \\\n",
       "0  /media/geoff/datasets/soapies_balanced_corpora...   \n",
       "1  /media/geoff/datasets/soapies_balanced_corpora...   \n",
       "2  /media/geoff/datasets/soapies_balanced_corpora...   \n",
       "3  /media/geoff/datasets/soapies_balanced_corpora...   \n",
       "4  /media/geoff/datasets/soapies_balanced_corpora...   \n",
       "\n",
       "                                                tgts  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/media/geoff/datasets/soapies_balanced_corpora/cs_engzul_balanced/lang_targs_mult/cs_engzul_trn.pkl'\n",
    "df = pd.read_pickle(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = df.iloc[8000]['audio_fpath']\n",
    "tgts1 = torch.tensor(df.iloc[8000]['tgts'])\n",
    "waveform1, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "audio_path = df.iloc[8001]['audio_fpath']\n",
    "tgts2 = torch.tensor(df.iloc[8001]['tgts'])\n",
    "waveform2, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "waveforms = pad_sequence([waveform1.reshape(-1, 1), waveform2.reshape(-1, 1)], batch_first=True)\n",
    "ll = torch.tensor([waveform1.size(1), waveform2.size(1)])\n",
    "targets = pad_sequence([tgts1.reshape(-1, 1), tgts2.reshape(-1, 1,)], padding_value=0, batch_first=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, lengths = model.forward(waveforms.squeeze(), ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_targets(targets, max_length):\n",
    "    targets_ = targets.reshape(targets.size(0), 1, 1, targets.size(1)).to(torch.float32)\n",
    "    interp_legnths = (1, max_length)\n",
    "    ds_targets = F.interpolate(targets_, interp_legnths).squeeze()\n",
    "    return ds_targets.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0835, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = interp_targets(targets, torch.max(lengths))\n",
    "F.cross_entropy(y_hat.view(-1, 3), targets.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_factor = 335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacky way to use torch's vision interpolation stuff\n",
    "\n",
    "alpha = 0.49 # Shrink factor to get ctc to work\n",
    "\n",
    "tgts = torch.tensor(tgts1, dtype=torch.float32).reshape(1, 1, 1, -1)\n",
    "ds_size1 = int(tgts.size(-1)/ds_factor*alpha)\n",
    "interp_legnths1 = (1,  ds_size1)\n",
    "interp_tgts1 = F.interpolate(tgts, interp_legnths1).squeeze()\n",
    "\n",
    "tgts = torch.tensor(tgts2, dtype=torch.float32).reshape(1, 1, 1, -1)\n",
    "ds_size2 = int(tgts.size(-1)/ds_factor*alpha)\n",
    "interp_legnths2 = (1,  ds_size2)\n",
    "interp_tgts2 = F.interpolate(tgts, interp_legnths2).squeeze()\n",
    "\n",
    "target = pad_sequence([interp_tgts1.reshape(-1, 1), interp_tgts2.reshape(-1, 1)], batch_first=True, padding_value=1).squeeze()\n",
    "target_lengths = torch.stack([torch.tensor(ds_size1), torch.tensor(ds_size2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.to(torch.long)\n",
    "input = F.log_softmax(y_hat, dim=-1)\n",
    "N, T, C = input.shape\n",
    "input = input.view(((T, N, C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7792, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "ctc_loss = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "ctc_loss(input, target, lengths, target_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 48\n",
    "C = 3\n",
    "N = 2\n",
    "\n",
    "S = 30\n",
    "S_min = 29\n",
    "\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "\n",
    "ctc_loss = torch.nn.CTCLoss(blank=0, zero_infinity=False)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoff/anaconda3/envs/penguin/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from utils.datasets import CSDataset, collator\n",
    "from torch.utils.data import DataLoader\n",
    "from models.lit_cs_detector import LitCSDetector\n",
    "\n",
    "path = '/media/geoff/datasets/soapies_balanced_corpora/cs_engzul_balanced/lang_targs_mult/cs_engzul_trn.pkl'\n",
    "df_trn = pd.read_pickle(path)\n",
    "path = '/media/geoff/datasets/soapies_balanced_corpora/cs_engzul_balanced/lang_targs_mult/cs_engzul_dev.pkl'\n",
    "df_dev = pd.read_pickle(path)\n",
    "path = '/media/geoff/datasets/soapies_balanced_corpora/cs_engzul_balanced/lang_targs_mult/cs_engzul_tst.pkl'\n",
    "df_tst = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS_SIZE = 3\n",
    "df_trn.tgts = df_trn.tgts-1\n",
    "df_dev.tgts = df_dev.tgts-1\n",
    "dataset_trn = CSDataset(df_trn)\n",
    "dataset_dev = CSDataset(df_dev)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_trn, batch_size=BS_SIZE, shuffle=True, collate_fn=collator, num_workers=12)\n",
    "dev_dataloader = DataLoader(dataset_dev, batch_size=BS_SIZE, collate_fn=collator, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitCSDetector(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "learning_rate_callback = LearningRateMonitor(logging_interval='step') \n",
    "checkpoint_callback = ModelCheckpoint(monitor='val/val_acc',\n",
    "                                        filename='{epoch}-{val/val_loss:.2f}-{val/val_auc:.2f}',\n",
    "                                        save_on_train_epoch_end=False,\n",
    "                                        auto_insert_metric_name=False,\n",
    "                                        save_last=True,\n",
    "                                        mode='max'\n",
    "                                        )\n",
    "callbacks = [learning_rate_callback, checkpoint_callback]\n",
    "\n",
    "# Logger                               \n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"../logs/\")\n",
    "trainer = pl.Trainer(logger=tb_logger, callbacks=callbacks, max_epochs=32, gpus=1, gradient_clip_val=0.5, accumulate_grad_batches=8, log_every_n_steps=100, precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = trainer.tuner.lr_find(model, train_dataloader, dev_dataloader)\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams.learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type          | Params\n",
      "----------------------------------------------------\n",
      "0 | feature_extractor | Wav2Vec2Model | 94.4 M\n",
      "1 | head              | Sequential    | 3.2 M \n",
      "----------------------------------------------------\n",
      "97.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "97.5 M    Total params\n",
      "195.058   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 30/2869 [00:05<09:02,  5.23it/s, loss=0.738, v_num=1] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, dev_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('penguin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a773ee532772c8f133f0d0eab8ce5b01098705323da0f49cbda586aadf84c18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
