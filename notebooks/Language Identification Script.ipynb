{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3ee544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preamble\n",
    "import os\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c728c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/media/geoff/datasets/soapies_balanced_corpora/\"\n",
    "transcriptions_file = work_dir + \"soapiescorpus_transcription_status_20170710.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23a1f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a598a35",
   "metadata": {},
   "source": [
    "### Create dictionary for the episode num, speaker name, and utterance number and the utterance ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0ea002",
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_to_id = {}\n",
    "id_to_speak = {}\n",
    "\n",
    "for lang_pair in [\"zul\",\"xho\",\"sot\",\"tsn\"]:\n",
    "    # speakers_to_id_file = work_dir + \"cs_eng\" + lang_pair + \"_balanced/linksmaps_ids.no_s.txt\"\n",
    "    speakers_to_id_file = work_dir + \"cs_eng\" + lang_pair + \"_balanced/transcriptions/linksmaps_ids.txt\"\n",
    "    with open(speakers_to_id_file) as f:\n",
    "        for line in f:\n",
    "            #speaker = line.split(\"_\")[0]\n",
    "            #ep_id = line.split(\"_\")[1]\n",
    "            #utt_num = line.split()[0].split(\"_\")[2]\n",
    "            utt_id_from_episode_info = line.split()[0]\n",
    "            \n",
    "            utt_id = line.split()[1].rstrip()\n",
    "            \n",
    "            speak_to_id[(utt_id_from_episode_info)] = (utt_id,lang_pair)\n",
    "            id_to_speak[(utt_id,lang_pair)] = (utt_id_from_episode_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b215ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54393"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(speak_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d6422",
   "metadata": {},
   "source": [
    "### List of all utterances with language and transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de09265",
   "metadata": {},
   "outputs": [],
   "source": [
    "trancriptions_w_utt_inf = {}\n",
    "\n",
    "with open(os.path.join(work_dir,transcriptions_file)) as f:\n",
    "    for line in f:\n",
    "        if line[0] == \"#\":\n",
    "            pass\n",
    "        else:\n",
    "            ep_id = \"_\".join(line.split()[0].split(\"_\")[1:])\n",
    "            speaker = line.split()[1]\n",
    "            utt_num = line.split()[2]\n",
    "            seg_stats = line.split()[9:]\n",
    "            \n",
    "            utt_id_from_episode_info = speaker + \"_\" + ep_id + \"_\" + utt_num\n",
    "            \n",
    "            #print(ep_id)\n",
    "            #print(speaker)\n",
    "            #print(utt_num)\n",
    "            #print(seg_stats)\n",
    "            trancriptions_w_utt_inf[utt_id_from_episode_info] = seg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4056472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['705.0',\n",
       " '11696.0',\n",
       " '12401.0',\n",
       " 'Setswana',\n",
       " 'ah#o#sa#ntso#u#na#le#di',\n",
       " '907.0',\n",
       " '12401.0',\n",
       " '13308.0',\n",
       " 'English',\n",
       " 'feelings#for#Lungile']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trancriptions_w_utt_inf['STEVE_100_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad12614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['312.666666667', '438736.0', '439048.666667', 'isiZulu', 'uqale#nge-', '312.666666667', '439048.666667', '439361.333333', 'English', 'toilet', '312.666666667', '439361.333333', '439674.0', 'isiZulu', 'ntwana']\n",
      "BASH_13-08-02_105\n"
     ]
    }
   ],
   "source": [
    "print(trancriptions_w_utt_inf[id_to_speak[('s00004_u00327','zul')]])\n",
    "print(id_to_speak[('s00004_u00327','zul')])\n",
    "#14-01-06 MAMPHO 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6081a079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['386.0',\n",
       " '633416.0',\n",
       " '633802.0',\n",
       " 'English',\n",
       " 'at#least',\n",
       " '598.0',\n",
       " '633802.0',\n",
       " '634400.0',\n",
       " 'isiZulu',\n",
       " 'zamanyana']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trancriptions_w_utt_inf[id_to_speak[('s00001_u00836','zul')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4446ce63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trancriptions_w_utt_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f9880",
   "metadata": {},
   "source": [
    "### Create files of utt lang information for the balanced corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6681a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zul\n",
      "trn\n",
      "zul\n",
      "dev\n",
      "zul\n",
      "tst\n",
      "xho\n",
      "trn\n",
      "xho\n",
      "dev\n",
      "xho\n",
      "tst\n",
      "sot\n",
      "trn\n",
      "sot\n",
      "dev\n",
      "sot\n",
      "tst\n",
      "tsn\n",
      "trn\n",
      "tsn\n",
      "dev\n",
      "tsn\n",
      "tst\n"
     ]
    }
   ],
   "source": [
    "multi_targ = True\n",
    "\n",
    "for lang_pair in [\"zul\",\"xho\",\"sot\",\"tsn\"]:#,,\"sot\"]: #zul\n",
    "    audio_root_folder = work_dir + \"cs_eng\" + lang_pair + \"_balanced\"\n",
    "    #audio_list_file = os.path.join(audio_root_folder,\"lists_unbalanced\")\n",
    "    audio_list_file = os.path.join(audio_root_folder,\"lists\")\n",
    "    if multi_targ:\n",
    "        audio_lang_targ_file = os.path.join(audio_root_folder,\"lang_targs_mult\")\n",
    "    else:\n",
    "        audio_lang_targ_file = os.path.join(audio_root_folder,\"lang_targs\")\n",
    "    \n",
    "    if not os.path.isdir(audio_lang_targ_file):\n",
    "        os.mkdir(audio_lang_targ_file)\n",
    "        \n",
    "    for set in [\"trn\",\"dev\",\"tst\"]:\n",
    "        print(lang_pair)\n",
    "        print(set)\n",
    "        with open(os.path.join(audio_list_file,set + \".lst\")) as f:\n",
    "            for line in f: \n",
    "                utt_lang_target = []\n",
    "                \n",
    "                # get ids in terms of speaker and episode\n",
    "                utt_id = ('s'+line.split(\"/s\")[1].split(\".\")[0],lang_pair)\n",
    "                speak_id = id_to_speak[utt_id]\n",
    "                #print(utt_id, \" -> \", speak_id)\n",
    "                \n",
    "                # get utterance information\n",
    "                if speak_id in list(trancriptions_w_utt_inf.keys()): # Would appear I have some transcriptions missing\n",
    "                    seg_stats = trancriptions_w_utt_inf[speak_id]\n",
    "                    segs = len(seg_stats) // 5\n",
    "                    frames_total = 0\n",
    "                    frames_prev = 0\n",
    "                    #print(seg_stats)\n",
    "                    for i in range(segs):\n",
    "                        frames_total += float(seg_stats[i*5])*16\n",
    "                        frames_new = round(frames_total) - round(frames_prev) # dur*f_s/1000 = frames\n",
    "                        lang = seg_stats[3+i*5]\n",
    "                        \n",
    "                        #if not frames.is_integer():\n",
    "                        #    raise Exception(\"Error non integer number frames, got:\",frames)\n",
    "                        if multi_targ:\n",
    "                            if seg_stats[3+i*5] == \"English\":\n",
    "                                for j in range(frames_new): \n",
    "                                    utt_lang_target.append(1)\n",
    "                            elif seg_stats[3+i*5] == \"isiZulu\":\n",
    "                                for j in range(frames_new):\n",
    "                                    utt_lang_target.append(2)\n",
    "                            elif seg_stats[3+i*5] == \"isiXhosa\":\n",
    "                                for j in range(frames_new):\n",
    "                                    utt_lang_target.append(3)\n",
    "                            elif seg_stats[3+i*5] == \"Sesotho\":\n",
    "                                for j in range(frames_new):\n",
    "                                    utt_lang_target.append(4)\n",
    "                            elif seg_stats[3+i*5] == \"Setswana\":\n",
    "                                for j in range(frames_new):\n",
    "                                    utt_lang_target.append(5)\n",
    "                        else:\n",
    "                            if seg_stats[3+i*5] == \"English\":\n",
    "                                for j in range(frames_new): \n",
    "                                    utt_lang_target.append(1)\n",
    "                            else:\n",
    "                                for j in range(frames_new):\n",
    "                                    utt_lang_target.append(2)\n",
    "\n",
    "                        frames_prev = frames_total\n",
    "                                \n",
    "                    #print(len(utt_lang_target))\n",
    "                    \n",
    "                    # write utt lang targs to file\n",
    "                    audio_lang_targ_speaker_file = os.path.join(audio_lang_targ_file,utt_id[0].split(\"_\")[0])\n",
    "                    \n",
    "                    if not os.path.isdir(audio_lang_targ_speaker_file):\n",
    "                        os.mkdir(audio_lang_targ_speaker_file)\n",
    "                        \n",
    "                    with open(os.path.join(audio_lang_targ_speaker_file,(utt_id[0] + \".txt\")),\"w\") as f:\n",
    "                        joined = \"\".join(str(x) for x in utt_lang_target)\n",
    "                            \n",
    "                        f.write(joined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('penguin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f5ccf0edd792a46bb8f186cfaeaecd18c8446d3eb40ff41bb5151f9e3e53f00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
